{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ ResNet-50 åœ¨ Imagenette æ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»è®­ç»ƒ\n",
    "\n",
    "## ğŸ“‹ ç›®å½•\n",
    "1. [ç¯å¢ƒè®¾ç½®](#1-ç¯å¢ƒè®¾ç½®)\n",
    "2. [åŠ è½½æ•°æ®é›†](#2-åŠ è½½æ•°æ®é›†)\n",
    "3. [åŠ è½½æ¨¡å‹](#3-åŠ è½½æ¨¡å‹)\n",
    "4. [æ•°æ®é¢„å¤„ç†](#4-æ•°æ®é¢„å¤„ç†)\n",
    "5. [é…ç½®è®­ç»ƒå‚æ•°](#5-é…ç½®è®­ç»ƒå‚æ•°)\n",
    "6. [å¼€å§‹è®­ç»ƒ](#6-å¼€å§‹è®­ç»ƒ)\n",
    "7. [æ¨¡å‹è¯„ä¼°](#7-æ¨¡å‹è¯„ä¼°)\n",
    "8. [ç»“æœå¯è§†åŒ–](#8-ç»“æœå¯è§†åŒ–)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "### 1.1 å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 é…ç½®ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ¬åœ°èµ„æºè·¯å¾„\n",
    "CACHE_DIR = \"./cache\"\n",
    "DATASET_PATH = os.path.join(CACHE_DIR, \"datasets\", \"imagenette\")\n",
    "MODEL_PATH = os.path.join(CACHE_DIR, \"models\", \"resnet-50\")\n",
    "RESULTS_DIR = \"./results\"\n",
    "\n",
    "# åˆ›å»ºç»“æœç›®å½•\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# è®¾ç½®ç¦»çº¿æ¨¡å¼\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜ï¼‰\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# å¯ç”¨ matplotlib å†…è”æ˜¾ç¤º\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆ!\")\n",
    "print(f\"ğŸ“ æ•°æ®é›†è·¯å¾„: {DATASET_PATH}\")\n",
    "print(f\"ğŸ¤– æ¨¡å‹è·¯å¾„: {MODEL_PATH}\")\n",
    "print(f\"ğŸ’¾ ç»“æœä¿å­˜ç›®å½•: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 è¾…åŠ©å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(text):\n",
    "    \"\"\"æ‰“å°ç¾åŒ–çš„æ ‡é¢˜\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {text}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def print_info(text, emoji=\"â„¹ï¸\"):\n",
    "    \"\"\"æ‰“å°æç¤ºä¿¡æ¯\"\"\"\n",
    "    print(f\"{emoji}  {text}\")\n",
    "\n",
    "def print_success(text):\n",
    "    \"\"\"æ‰“å°æˆåŠŸä¿¡æ¯\"\"\"\n",
    "    print(f\"âœ… {text}\")\n",
    "\n",
    "def print_warning(text):\n",
    "    \"\"\"æ‰“å°è­¦å‘Šä¿¡æ¯\"\"\"\n",
    "    print(f\"âš ï¸  {text}\")\n",
    "\n",
    "print(\"âœ… è¾…åŠ©å‡½æ•°å®šä¹‰å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. åŠ è½½æ•°æ®é›†\n",
    "\n",
    "åŠ è½½æœ¬åœ°ç¼“å­˜çš„ Imagenette æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"ğŸ“Š åŠ è½½æ•°æ®é›†\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print_warning(\"æ•°æ®é›†æœªæ‰¾åˆ°! è¯·å…ˆè¿è¡Œ: python download_resources.py\")\n",
    "    raise FileNotFoundError(f\"æ•°æ®é›†ä¸å­˜åœ¨: {DATASET_PATH}\")\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯\n",
    "print_info(\"æ•°æ®é›†ç»“æ„:\", \"ğŸ“¦\")\n",
    "print(dataset)\n",
    "\n",
    "# ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"  â€¢ {split:12s}: {len(dataset[split]):,} æ ·æœ¬\")\n",
    "\n",
    "print_success(\"æ•°æ®é›†åŠ è½½å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 æŸ¥çœ‹æ ·æœ¬ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬å›¾åƒ\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Dataset Sample Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample = dataset['train'][idx]\n",
    "    ax.imshow(sample['image'])\n",
    "    ax.set_title(f\"Label: {sample['label']}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æ ·æœ¬å›¾åƒå±•ç¤ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. åŠ è½½æ¨¡å‹\n",
    "\n",
    "åŠ è½½é¢„è®­ç»ƒçš„ ResNet-50 æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 åŠ è½½å›¾åƒå¤„ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"ğŸ¤– åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨\")\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print_warning(\"æ¨¡å‹æœªæ‰¾åˆ°! è¯·å…ˆè¿è¡Œ: python download_resources.py\")\n",
    "    raise FileNotFoundError(f\"æ¨¡å‹ä¸å­˜åœ¨: {MODEL_PATH}\")\n",
    "\n",
    "# åŠ è½½å›¾åƒå¤„ç†å™¨\n",
    "print_info(\"æ­£åœ¨åŠ è½½å›¾åƒå¤„ç†å™¨...\", \"âš™ï¸\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print_success(\"å›¾åƒå¤„ç†å™¨åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è·å–æ ‡ç­¾ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ ‡ç­¾ä¿¡æ¯\n",
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "num_labels = len(labels)\n",
    "\n",
    "label2id = {label: str(i) for i, label in enumerate(labels)}\n",
    "id2label = {str(i): label for i, label in enumerate(labels)}\n",
    "\n",
    "print_info(f\"ç±»åˆ«æ•°é‡: {num_labels}\", \"ğŸ·ï¸\")\n",
    "print_info(f\"ç±»åˆ«åˆ—è¡¨:\", \"ğŸ“\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(\"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ResNet-50...\", \"ğŸ§ \")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# ç»Ÿè®¡æ¨¡å‹å‚æ•°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print_info(f\"æ¨¡å‹æ€»å‚æ•°: {total_params:,}\", \"ğŸ“Š\")\n",
    "print_info(f\"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\", \"ğŸ¯\")\n",
    "print_success(\"æ¨¡å‹åŠ è½½å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†(è°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–ç­‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"ğŸ¨ æ•°æ®é¢„å¤„ç†\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"æ•°æ®é¢„å¤„ç†å‡½æ•°\"\"\"\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    inputs = image_processor(images)\n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "# é¢„å¤„ç†æ•°æ®é›†\n",
    "prepared_ds = {}\n",
    "for split in dataset.keys():\n",
    "    print_info(f\"å¤„ç† {split} é›†...\", \"âš™ï¸\")\n",
    "    prepared_ds[split] = dataset[split].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"image\"],\n",
    "        desc=f\"é¢„å¤„ç† {split} é›†\"\n",
    "    )\n",
    "    prepared_ds[split].set_format(\"torch\")\n",
    "    print_success(f\"{split} é›†é¢„å¤„ç†å®Œæˆ ({len(prepared_ds[split])} æ ·æœ¬)\")\n",
    "\n",
    "print_success(\"æ‰€æœ‰æ•°æ®é¢„å¤„ç†å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. é…ç½®è®­ç»ƒå‚æ•°\n",
    "\n",
    "è®¾ç½®è®­ç»ƒé…ç½®(æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è½®æ•°ç­‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°\")\n",
    "\n",
    "# è®­ç»ƒé…ç½® - å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "\n",
    "print_info(f\"æ‰¹æ¬¡å¤§å° (è®­ç»ƒ): {train_batch_size}\", \"ğŸ“¦\")\n",
    "print_info(f\"æ‰¹æ¬¡å¤§å° (è¯„ä¼°): {eval_batch_size}\", \"ğŸ“¦\")\n",
    "print_info(f\"è®­ç»ƒè½®æ•°: {num_epochs}\", \"ğŸ”„\")\n",
    "print_info(f\"å­¦ä¹ ç‡: {learning_rate}\", \"ğŸ“ˆ\")\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒæ­¥æ•°\n",
    "total_steps = (len(prepared_ds[\"train\"]) // train_batch_size) * num_epochs\n",
    "print_info(f\"é¢„è®¡è®­ç»ƒæ­¥æ•°: {total_steps:,}\", \"â±ï¸\")\n",
    "\n",
    "# å®šä¹‰è¯„ä¼°æŒ‡æ ‡\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resnet50-imagenette\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    eval_strategy=\"no\",  # è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¿›è¡ŒéªŒè¯\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=os.path.join(RESULTS_DIR, \"logs\"),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print_success(\"è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "åˆå§‹åŒ– Trainer å¹¶å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "> âš ï¸ **æ³¨æ„**: è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´(å–å†³äºç¡¬ä»¶é…ç½®)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"ğŸš€ å¼€å§‹è®­ç»ƒ\")\n",
    "\n",
    "# åˆå§‹åŒ– Trainerï¼ˆ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print_success(\"Trainer åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "# è®°å½•å¼€å§‹æ—¶é—´\n",
    "start_time = datetime.now()\n",
    "print_info(f\"è®­ç»ƒå¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\", \"â°\")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "train_result = trainer.train()\n",
    "\n",
    "# è®°å½•ç»“æŸæ—¶é—´\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "print_success(f\"è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {training_duration}\")\n",
    "\n",
    "# æ˜¾ç¤ºè®­ç»ƒæŒ‡æ ‡\n",
    "train_metrics = train_result.metrics\n",
    "print(\"\\nğŸ“Š è®­ç»ƒæŒ‡æ ‡:\")\n",
    "for key, value in train_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  â€¢ {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. æ¨¡å‹è¯„ä¼°\n",
    "\n",
    "åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"ğŸ“‹ æ¨¡å‹è¯„ä¼°\")\n",
    "\n",
    "# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°\n",
    "print_info(\"åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°...\", \"ğŸ”\")\n",
    "eval_results = trainer.evaluate(prepared_ds[\"validation\"])\n",
    "\n",
    "print(\"\\nğŸ“Š éªŒè¯é›†æŒ‡æ ‡:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  â€¢ {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "print_success(\"è¯„ä¼°å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 ç”Ÿæˆé¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆé¢„æµ‹ç”¨äºå¯è§†åŒ–\n",
    "print_info(\"ç”Ÿæˆé¢„æµ‹ç»“æœ...\", \"ğŸ¯\")\n",
    "\n",
    "eval_dataset = prepared_ds[\"validation\"]\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "print_success(f\"é¢„æµ‹å®Œæˆ! å…± {len(pred_labels)} ä¸ªæ ·æœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ç»“æœå¯è§†åŒ–\n",
    "\n",
    "ç”Ÿæˆå¤šç»´åº¦çš„å¯è§†åŒ–å›¾è¡¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 æ··æ·†çŸ©é˜µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æ··æ·†çŸ©é˜µç»˜åˆ¶å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 å„ç±»åˆ«å‡†ç¡®ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡\n",
    "class_accuracy = []\n",
    "for i in range(len(labels)):\n",
    "    mask = true_labels == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (pred_labels[mask] == true_labels[mask]).mean()\n",
    "        class_accuracy.append(acc * 100)\n",
    "    else:\n",
    "        class_accuracy.append(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(labels)))\n",
    "bars = plt.barh(labels, class_accuracy, color=colors)\n",
    "plt.xlabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, (bar, acc) in enumerate(zip(bars, class_accuracy)):\n",
    "    plt.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ç±»åˆ«å‡†ç¡®ç‡å›¾è¡¨ç»˜åˆ¶å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 æ•´ä½“æ€§èƒ½æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—å„é¡¹æŒ‡æ ‡\n",
    "precision = precision_score(true_labels, pred_labels, average='weighted') * 100\n",
    "recall = recall_score(true_labels, pred_labels, average='weighted') * 100\n",
    "f1 = f1_score(true_labels, pred_labels, average='weighted') * 100\n",
    "accuracy = eval_results['eval_accuracy'] * 100\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics, values, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Score (%)', fontsize=12)\n",
    "plt.title('Overall Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{value:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æ€§èƒ½æŒ‡æ ‡å›¾è¡¨ç»˜åˆ¶å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 çœŸå® vs é¢„æµ‹åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»Ÿè®¡åˆ†å¸ƒ\n",
    "true_counts = np.bincount(true_labels, minlength=len(labels))\n",
    "pred_counts = np.bincount(pred_labels, minlength=len(labels))\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, true_counts, width, label='True Distribution', color='skyblue', alpha=0.8)\n",
    "plt.bar(x + width/2, pred_counts, width, label='Predicted Distribution', color='lightcoral', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Sample Count', fontsize=12)\n",
    "plt.title('True vs Predicted Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, labels, rotation=45, ha='right')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… åˆ†å¸ƒå¯¹æ¯”å›¾è¡¨ç»˜åˆ¶å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\")\n",
    "print(\"=\" * 70)\n",
    "report = classification_report(true_labels, pred_labels, target_names=labels, digits=4)\n",
    "print(report)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•´ä½“å‡†ç¡®ç‡: {accuracy:.2f}%\")\n",
    "print(f\"ğŸ“Š åŠ æƒç²¾ç¡®ç‡: {precision:.2f}%\")\n",
    "print(f\"ğŸ“Š åŠ æƒå¬å›ç‡: {recall:.2f}%\")\n",
    "print(f\"ğŸ“Š åŠ æƒF1åˆ†æ•°: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 ä¿å­˜å®Œæ•´å¯è§†åŒ–å›¾è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º 2x2 ç»¼åˆå›¾è¡¨\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. æ··æ·†çŸ©é˜µ\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 2. å„ç±»åˆ«å‡†ç¡®ç‡\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "bars = plt.barh(labels, class_accuracy, color=plt.cm.viridis(np.linspace(0, 1, len(labels))))\n",
    "plt.xlabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlim(0, 100)\n",
    "for i, (bar, acc) in enumerate(zip(bars, class_accuracy)):\n",
    "    plt.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "# 3. æ€§èƒ½æŒ‡æ ‡\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "bars = plt.bar(metrics, values, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Score (%)', fontsize=12)\n",
    "plt.title('Overall Performance Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{value:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 4. åˆ†å¸ƒå¯¹æ¯”\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "plt.bar(x - width/2, true_counts, width, label='True Distribution', color='skyblue', alpha=0.8)\n",
    "plt.bar(x + width/2, pred_counts, width, label='Predicted Distribution', color='lightcoral', alpha=0.8)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Sample Count', fontsize=12)\n",
    "plt.title('True vs Predicted Distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xticks(x, labels, rotation=45, ha='right')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜å›¾è¡¨\n",
    "plot_file = os.path.join(RESULTS_DIR, 'evaluation_results.png')\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print_success(f\"ç»¼åˆå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ JSON ç»“æœ\n",
    "results_summary = {\n",
    "    \"training_duration\": str(training_duration),\n",
    "    \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"train_metrics\": {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) \n",
    "                     for k, v in train_metrics.items()},\n",
    "    \"validation_metrics\": {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) \n",
    "                          for k, v in eval_results.items()},\n",
    "    \"num_labels\": num_labels,\n",
    "    \"labels\": labels,\n",
    "}\n",
    "\n",
    "results_file = os.path.join(RESULTS_DIR, \"training_results.json\")\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print_success(f\"JSON ç»“æœå·²ä¿å­˜åˆ°: {results_file}\")\n",
    "\n",
    "# ä¿å­˜åˆ†ç±»æŠ¥å‘Š\n",
    "report_file = os.path.join(RESULTS_DIR, 'classification_report.txt')\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Image Classification Report\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\\n\" + \"=\" * 70 + \"\\n\")\n",
    "    f.write(f\"Overall Accuracy: {accuracy:.2f}%\\n\")\n",
    "    f.write(f\"Weighted Precision: {precision:.2f}%\\n\")\n",
    "    f.write(f\"Weighted Recall: {recall:.2f}%\\n\")\n",
    "    f.write(f\"Weighted F1-Score: {f1:.2f}%\\n\")\n",
    "\n",
    "print_success(f\"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}\")\n",
    "print_info(f\"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {os.path.abspath(RESULTS_DIR)}\", \"ğŸ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ è®­ç»ƒå®Œæˆ!\n",
    "\n",
    "### ğŸ“Š ç»“æœæ€»ç»“\n",
    "\n",
    "- âœ… è®­ç»ƒå®Œæˆ\n",
    "- âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ\n",
    "- âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ\n",
    "- âœ… ç»“æœæ–‡ä»¶ä¿å­˜\n",
    "\n",
    "### ğŸ“ è¾“å‡ºæ–‡ä»¶\n",
    "\n",
    "æ‰€æœ‰ç»“æœä¿å­˜åœ¨ `results/` ç›®å½•:\n",
    "- `evaluation_results.png` - ç»¼åˆå¯è§†åŒ–å›¾è¡¨\n",
    "- `classification_report.txt` - è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "- `training_results.json` - JSON æ ¼å¼ç»“æœ\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "- å°è¯•è°ƒæ•´è¶…å‚æ•°æå‡æ€§èƒ½\n",
    "- ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†\n",
    "- å°è¯•å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹\n",
    "- éƒ¨ç½²æ¨¡å‹è¿›è¡Œæ¨ç†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
